«Китайська Кімната» – в’язниця для роботів

Із стрімким розвитком у галузі штучного інтелекту та реальними перспективами взаємодії з ним у багатьох царинах повсякденного життя, справедливо постають питання про те, як варто себе з ним поводити. Ці питання {простягаються=>виходять:::error_type=F/Style} далеко за межі практичного {та=>і:::error_type=F/Repetition} сягають морально-філософських проблем, що постають перед нами, коли ми уявляємо собі предмет, який поводиться ніби {він =>:::error_type=F/PoorFlow}живий. Багато хто вже ставиться до роботів як до живих істот, а дехто згадує славнозвісний тест {Тюринґа=>Тюрінґа:::error_type=Spelling}, коли {роботу=>роботові:::error_type=F/Style} можна приписувати людську свідомість, якщо після спілкування з ним складеться враження, ніби він {=>— :::error_type=Punctuation}людина. Але питання визначення такої свідомості насправді дуже складне і вже більш як пів століття викликає запеклі суперечки. Аби продемонструвати, що про зміст не завжди можна судити за формою, я розповім про так звану {Проблему=>проблему:::error_type=Spelling} {Китайської=>китайської:::error_type=Spelling} {Кімнати=>кімнати:::error_type=Spelling}. {Вона була=>Її:::error_type=F/PoorFlow} {сформована=>сформулював:::error_type=G/VerbVoice}{ у=>:::error_type=F/PoorFlow} 1980 {році=>року:::error_type=G/Case} {американським=>американський:::error_type=G/Case} {філософом=>філософ:::error_type=G/Case} {Джоном=>Джон:::error_type=G/Case} {Роджерсом=>Роджерс:::error_type=G/Case} {Серлом=>Серл:::error_type=G/Case} (John Rogers Searle){ і вийшла вона=>, у:::error_type=F/PoorFlow} {статею=>статті:::error_type=G/Case} „Minds, Brains, and Programs“ у журналі „The Behavioral and Brain Sciences“. Палкі дискусії почалися ще перед виходом статті, тож у її остаточній версії автор опублікував відповіді на найважливішу критику. Та можливі відповіді на цю {Проблему=>проблему:::error_type=Spelling} публікують і досі. Формулюючи суть, я залишив за собою свободу переформулювати умови задачі так, аби вони були зрозумілі сучасному читачеві.

Уявіть, що Вас {взяли=>узяли:::error_type=Spelling} на роботу як секретаря, замкнули у кімнаті, заповненій книгами, та наказали Вам вести переписку з клієнтами. Звучить як типовий робочий день в офісі, {еге-ж=>еге ж:::error_type=Spelling}? Але є одна проблема. Ваші клієнти спілкуються мовою, яку ви ніколи не вчили і не розумієте. Джон Серл{,=>:::error_type=Punctuation} не розумів китайської{,=>:::error_type=Punctuation} і вів {оповідання=>оповідь:::error_type=F/Style} від своєї особи {–=>—:::error_type=Punctuation} {звідти=>звідси:::error_type=F/Style} {=>й :::error_type=F/PoorFlow}назва мисленого експерименту. Ваш робочий процес виглядає так: Ви отримуєте записку чи повідомлення, яке написано ієрогліфами та маєте на нього відповісти. Щоб це зробити, {Вам=>вам:::error_type=Spelling} на допомогу приходять книги, якими обставлена кімната {–=>—:::error_type=Punctuation} вони виявляються довідниками з інструкцій. Ви шукаєте комбінацію ієрогліфів {з=>із:::error_type=Spelling} повідомлення у довіднику та дивитеся, які ієрогліфи потрібно написати у відповідь. І ось {–=>—:::error_type=Punctuation} {Ваш=>ваш:::error_type=Spelling} клієнт отримав те, про що питав, і йде задоволений. Можливо, він {спитав=>запитав:::error_type=F/Style} {Вас=>вас:::error_type=Spelling}{, який=> про:::error_type=F/PoorFlow} ваш улюблений колір, можливо {–=>—:::error_type=Punctuation} про атомну масу диспрозію {–=>—:::error_type=Punctuation} ви цього не дізнаєтеся, бо не розумієте його мови. Але для клієнта це виглядатиме так, ніби він говорить з людиною, яка якщо й не розбирається в поставлених питаннях, то хоча б розмовляє китайською. У нього є всі підстави вважати, що якщо щось виглядає як качка та крякає {–=>—:::error_type=Punctuation} це є качка.

З точки зору програмування штучного інтелекту, питання клієнта {–=>—:::error_type=Punctuation} це інструкція, а {Ваша=>ваша:::error_type=Spelling} відповідь {–=>—:::error_type=Punctuation} це реакція ШІ на інструкцію. Довідники, що ними заповнена кімната, можуть бути видані начальством {–=>—:::error_type=Punctuation} тоді ми кажемо, що ШІ *запрограмовано* згори, а може бути, що {Ви=>ви:::error_type=Spelling} самі їх заповнюєте, коли спілкуєтеся з клієнтом. В такому разі, клієнт, наприклад, має поставити оцінку реалістичності {Вашої=>вашої:::error_type=Spelling} відповіді. Поспілкувавшись кілька мільйонів разів, {Ви=>ви:::error_type=Spelling} (чи {Ваш=>ваш:::error_type=Spelling} відділ {–=>—:::error_type=Punctuation} роботу, дяка Богу, можна розділити) зможете записати у довідники відповіді, за які було отримано найвищий бал; але робитимете {Ви=>ви:::error_type=Spelling} це навмання, випадково підбираючи варіанти. Це називається *машинним навчанням*. Як бачите, усі ці процеси {–=>—:::error_type=Punctuation} це лише реакція на зовнішні команди, а розуміння мови для цього не є потрібним. Аби впевнитися, що стелажі зі словниками самі не мають розуміння мови (аргумент про системність), {Ви=>ви:::error_type=Spelling} можете зазубрити усі відповіді з усіх книжок напам’ять {–=>—:::error_type=Punctuation} та відповідати самотужки. Ви все ще не розумієте китайської та все ще виглядаєте так, ніби розумієте.

Такий принцип можна відтворити у будь-чому. «Паперова машина {Тюринґа=>Тюрінга:::error_type=Spelling}» {-=>—:::error_type=Punctuation} це набір інструкцій для гри у шахи, де написано, який хід треба робити у випадку будь-якої певної розстановки фігур на дошці. Той, хто читає цю інструкцію, не повинен вміти грати в шахи. {Можливо=>Можна:::error_type=F/Style} навіть збудувати штучний інтелект, який може сам навчитися грати у хрестики-нулики, маючи сотню сірникових коробок та кількасот жетонів різного кольору. Про нього я, можливо, ще розповім. Але принцип в усіх цих випадках один і той самий, і за ним виконання дій не означає їхнього розуміння.

Наразі{,=>:::error_type=Punctuation} ми маємо роботів Boston Dynamics, які ще вчаться ходити та взаємодіяти з навколишніми предметами. Маємо ми і робота Софію, яка вміє слухати та говорити, зчитувати та показувати емоції. Але варто розуміти, що людську поведінку можна імітувати, не будучи людиною та не маючи самосвідомості, і це {все-одно=>все одно:::error_type=Spelling} виглядатиме справжнім. Сучасні технології взагалі добре опанували імітацію. Головна різниця між людиною та машиною, яку треба зазначити на цьому етапі {–=>—:::error_type=Punctuation} це те, що людина може покинути китайську кімнату та вивчити мову будь-яким відомим методом. Штучний інтелект не може вийти з кімнати {–=>—:::error_type=Punctuation} все, що {поступає=>надходить:::error_type=F/Style} до нього, переводиться у двійковий код {–=>—:::error_type=Punctuation} його «ієрогліфи», і лише ними він «бачить» світ.

Ознака, яку має інтелект людини чи інших тварин натомість, зветься розумним словом {=>":::error_type=Punctuation}інтенціональність{=>":::error_type=Punctuation}. Воно означає, що розумова діяльність істоти направлена на щось та має певну ціль. Інтенціональність, {як то =>:::error_type=F/PoorFlow}можна сказати, обумовлена конструкційно. Найпростіше її зрозуміти, поглянувши на найпростіші емоції на кшталт голоду та страху. Нервова система тварин збудована так, аби розуміти небезпеку {–=>—:::error_type=Punctuation} ба{,=>:::error_type=Punctuation} навіть проста жива клітина, яка насправді є мініатюрним комп’ютером, також має таке розуміння. Воно зумовлене наявністю певного стану рівноваги клітини чи нервової системи, якого ці системи завжди прагнуть. {Все=>Усе:::error_type=Spelling}, що людина сприймає, як ззовні, так і зсередини, порушує такий стан рівноваги, і реакції людини напрямлені на те, аби цей стан коли-небудь віднайти. Це й навчило людину *розуміти*. Людина розуміє китайську, бо має з цього якийсь зиск {–=>—:::error_type=Punctuation} від виживання в Китаї до задоволення спілкуватися з друзями з Шанхаю. Китайська кімната {всього-лише=>всього лише:::error_type=Spelling} запрограмована відповідати на запити. Своєї інтенціональності, свого зиску вона не має, а зовнішні запити не збурюють її внутрішній стан. Іншими словами, як би {Ви=>ви:::error_type=Spelling} не відповідали клієнтам, {Вашу=>вашу:::error_type=Spelling} заробітну плату не змінять {–=>—:::error_type=Punctuation} треба лише відповідати за інструкціями.

Тому {передчасним=>передчасно:::error_type=F/Style} {є =>:::error_type=F/PoorFlow}приписувати штучним інтелектам будь-який вид людської свідомості лише на базі того, що вони успішно склали тест {Тюринґа=>Тюрінга:::error_type=Spelling}. Звісно, людям {притаманно=>властиво:::error_type=F/Style} приписувати людську сутність усьому, що виглядає як людина та поводить себе антропоморфно {–=>—:::error_type=Punctuation} це вже програма, закладена у наш мозок, і це непомітно для нас допомагає нам у соціалізації. Але одвічні питання наукової фантастики стосовно того, як люди повинні поводити себе з синтетичним життям, ще мають багато часу для опрацювання, бо до створення справжніх штучних істот буде ще багато складної та кропіткої роботи {–=>—:::error_type=Punctuation} і це ще якщо людство вирішить обрати цей шлях та не відступитися від нього. Якщо роботів з інтенціональністю створити й можна {–=>—:::error_type=Punctuation} окрім наукових дослідів{=>,:::error_type=Punctuation} важко уявити собі від них практичну користь, {яку=>якої:::error_type=G/Case} б не мали до того часу вже звичайні безвольні штучні інтелекти. Та навіть коли таке життя створити, його не варто поспішати оцінювати відомими категоріями. Багато хто бачив «знущання над роботами» під час тестів у Boston Dynamics, і багато кому певно було шкода тих роботів, яким робили боляче. Так от {–=>—:::error_type=Punctuation} роботам {все=>усе:::error_type=Spelling} одно{,=>:::error_type=Punctuation} і їм точно не боляче, бо для них не побудували нервової системи, яка б передавала біль чи фрустрацію. Настільки очевидно {–=>—:::error_type=Punctuation} але все одно доводиться говорити вголос. Більшість очікувань та страхів перед ШІ {–=>—:::error_type=Punctuation} це наші власні людські проєкції.

Тож, можливо, у недалекому майбутньому роботи й захоплять світ, якщо їх запрограмувати чи навчити, але вони навряд чи розумітимуть, для чого їм {все=>усе:::error_type=Spelling} це. Боятися треба не машин, а тих, хто ними користується.

~by Ivort
=====
[1] Searle J. Minds, Brains, and Programs. The Philosophy of Artificial Intelligence / Boden M (ed.) Oxford. 1990. Вперше опубліковано в журналі: «The Behavioral and Brain Sciences», 1980, № 3, pp. 417–424.
[2] M. Kaku (2008). Physics of the Impossible: A Scientific Exploration Into the World of Phasers, Force Fields, Teleportation, and Time Travel. Doubleday. ISBN 978-0-385-52069-0. Part I; 7.
[3] R. Dawkins (1976). The Selfish Gene. Oxford University Press.  ISBN 0-19-857519-X.
[4] Выход из «Китайской комнаты» или может ли машина думать. Допис користувача IvanBannikov на https://habr.com/ru/post/405941/
