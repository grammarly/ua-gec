S У цій доповіді ми розглянемо останні тренди в опрацюванні мовлення ( speech processing ) .
A -1 -1|||noop|||-NONE-|||-NONE-|||-NONE-|||1

S Головним з них є transfer learning .
A -1 -1|||noop|||-NONE-|||-NONE-|||-NONE-|||1

S При transfer learning ми спершу претренуємо модель на задачі , яка не обов'язково нас цікавить , але для якої є багато даних .
A -1 -1|||noop|||-NONE-|||-NONE-|||-NONE-|||1

S Потім ми беремо цю претреновану модель та донавчаємо ( fine-tuning ) її на задачі , яка справді нас цікавить , але для якої зазвичай є мало даних .
A -1 -1|||noop|||-NONE-|||-NONE-|||-NONE-|||1

S У текстовому NLP використовуються великі обсяги нерозміченого тексту для претренування мовних моделей , таких як GPT - 2 та BERT .
A -1 -1|||noop|||-NONE-|||-NONE-|||-NONE-|||1

S Для опрацювання мовлення ми можемо подібним чином використати нерозмічені звукові дані -- подкасти , відео з YouTube , аудіокнижки тощо .
A 11 12|||Punctuation|||—|||REQUIRED|||-NONE-|||1

S У текстовому NLP нашою задачею було або вгадувати наступне слово ( GPT - 3 ) , або замасковані слова ( BERT ) .
A 3 4|||G/Gender|||нашим|||REQUIRED|||-NONE-|||1

S Задача для претренінгу в speech processing є дуже подібною .
A -1 -1|||noop|||-NONE-|||-NONE-|||-NONE-|||1

S У нас тепер немає слів ( токенів ) , але є звукові фрейми .
A -1 -1|||noop|||-NONE-|||-NONE-|||-NONE-|||1

S Ми можемо або передбачати , які фрейми будуть слідувати за попередніми , або закривати маскою частину фреймів та намагатися відтворити їх з контексту .
A -1 -1|||noop|||-NONE-|||-NONE-|||-NONE-|||1

S Ці ідеї , у спрощенному вигляді , лежать в основі Contrastive predicting coding ( CPC ) , wav2vec 2 . 0 та HuBERT .
A 4 5|||Spelling|||спрощеному|||REQUIRED|||-NONE-|||1

S Деталі реалізації відрізняються для всіх цих моделей .
A -1 -1|||noop|||-NONE-|||-NONE-|||-NONE-|||1

S Але для всіх них є спільний " інтерфейс " : на вхід подається звуковий сигнал ( waveform ) , на виході маємо послідовність векторів , які цей сигнал кодують .
A -1 -1|||noop|||-NONE-|||-NONE-|||-NONE-|||1

S Кожен вектор відповідає маленькому часовому фрагменту ( 10 - 20 мс ) та містить в собі більше семантичної інформації , ніж просто вейвформа чи Mel - спектрограма .
A 14 15|||Spelling|||у|||REQUIRED|||-NONE-|||1

S З цих моделей найновішою та емпірично найкращою є HuBERT від Facebook AI Research .
A -1 -1|||noop|||-NONE-|||-NONE-|||-NONE-|||1

S Розгляньмо її трохи більш детально .
A -1 -1|||noop|||-NONE-|||-NONE-|||-NONE-|||1

