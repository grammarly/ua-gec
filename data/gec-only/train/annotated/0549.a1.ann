"Філософсько-етичні проблеми у сфері штучного інтелекту"
Питання життя та смерті {-=>—:::error_type=Punctuation} одвічне філософське питання, яке більшість людей {вирішую=>вирішує:::error_type=Spelling} {по своєму=>по-своєму:::error_type=Spelling}. А що{,=>:::error_type=Punctuation} {якщо=>коли:::error_type=G/Conjunction}{  => :::error_type=Punctuation}я скажу{=>,:::error_type=Punctuation} що це питання потребує негайного {і=>й:::error_type=Spelling} одноголосного рішення? 
За останні декілька років прогрес у сфері штучного інтелекту та машинного навчання зробив величезний крок {вперед=>уперед:::error_type=Spelling}. Більшість людей вже {звикли=>звикла:::error_type=G/Number} до смартових речей у нашому житті — {фітнес трекерів=>фітнес-трекерів:::error_type=Spelling} з вимірюванням пульсу, кардіограми та багатьох біометричних показників, смартфонів з розблокуванням сітківкою чи власним обличчям, для багатьох навіть роботи доставки {=>":::error_type=Punctuation}Амазон{=>":::error_type=Punctuation} на вулицях Силіконової долини {=>— :::error_type=Punctuation}звичайна річ. Насправді процес сягає великих проривів {в=>у:::error_type=Spelling} медицині. Зараз системи розпізнавання зображень здатні визначити наявність певних видів раку чи наявність пневмонії з якістю кращою {ніж=>за:::error_type=G/Prep} {кваліфіковані лікарі=>кваліфікованих лікарів:::error_type=G/Case}. Більшість {з=>із:::error_type=Spelling} нас не {задумуються=>задумується:::error_type=G/Number} про етичні та філософські проблеми{=>,:::error_type=Punctuation} з якими стикаються спеціалісти з штучного інтелекту, які розробляють технологічні рішення.
У серпні цього року науковий співробітник {з=>із:::error_type=Spelling} комп’ютерного зору та робототехніки університету {Кембрідж=>Кембридж:::error_type=Spelling} Алекс Кендал опублікував роздум ”Let`s talk about Artificial intelligence”, де він {розглядав=>розглянув:::error_type=G/Aspect} конкретні етичні проблеми спеціалістів {з=>із:::error_type=Spelling} машинного навчання. Три {ключових=>ключові:::error_type=G/Case} проблеми, які він розглянув, це: довіра, чесність та справедливість.
Якщо чесність вимагає в основному легкого інтерпретування рішень, тобто вміння алгоритмом надати певні суттєві причини для деякого рішення, то з довірою та справедливістю все набагато складніше. 
Справедливість. Розглянемо доволі простий приклад. Згідно з TrafficSTATS study by CMU for AAA (2007) чоловіки мають на 77% {вищій=>вищий:::error_type=Spelling} ризик померти в автомобільній аварії, ніж жінка. Тобто{,=>:::error_type=Punctuation} якщо подібні {данні=>дані:::error_type=Spelling} включають {в=>у:::error_type=Spelling} модель{=>,:::error_type=Punctuation} яка визначає {імовірність=>ймовірність:::error_type=Spelling} потрапити в аварію та вартість страхового полісу, вона матиме кращу точність, тобто буде справедливішою до реальної статистики. Проте, згідно з рішенням {Европейського=>Європейського:::error_type=Spelling} суду справедливості у 2011 році, стать не може бути врахована у вартість страхового полісу як фактор ризику, бо {вважається=>вважатиметься:::error_type=G/Tense} {дескримінацією=>дискримінацією:::error_type=Spelling}.
Іншими виявленими прикладами нечесних / необ’єктивних систем є: система розпізнавання голосу компанії Google систематично краще справляється з чоловічим голосом{=>,:::error_type=Punctuation} ніж {з=>із:::error_type=Spelling} жіночим, система підрахунку ризику повторного правопорушення необ’єктивна щодо {афро-американців=>афроамериканців:::error_type=Spelling}. 
{Нажаль=>На жаль:::error_type=Spelling}, системи штучного інтелекту мають властивість наслідувати упередженість від людей. Вирішення цієї проблеми полягає в використанні {більш якісних=>якісніших:::error_type=G/Comparison} датасетів для кращого вивчення маленьких (рідкісних) класів, збільшенні якості методів збору та обробки даних, для зменшення упередженості даних.
Якщо зі справедливістю кроки доволі технічні, то з довірою все набагато складніше. Критично важливим є те, що користувачі довіряють системам штучного інтелекту. Якщо система не має належного рівня довіри, то ми навряд будемо її використовувати. Для побудови безпечних систем машинного навчання{,=>:::error_type=Punctuation} потрібно показати високу точність алгоритму, будувати алгоритми, які не тільки мають високу точність вирішення задачі, а й {вміють=>уміють:::error_type=Spelling} визначати неточність своїх передбачень {та=>і:::error_type=Spelling} розуміють, що є речі, які їм {не відомі=>невідомі:::error_type=Spelling} (тобто, наприклад, використовувати Баєсівські моделі глибокого навчання).
Повертаючись до проблеми життя та смерті. {у=>У:::error_type=Spelling} 2015 році видання {=>":::error_type=Punctuation}MIT Technology {review=>Review:::error_type=Spelling}{=>":::error_type=Punctuation} опублікувало на тему “Why Self-Driving Cars Must Be Programmed to Kill”{.=>,:::error_type=Punctuation} У тілі, якої представлено високо ймовірну ситуацію на дорозі, де хтось з великою ймовірністю загине{=>,:::error_type=Punctuation} лишається тільки обрати{=>,:::error_type=Punctuation} хто саме. Саме тому{,=>:::error_type=Punctuation} на даний момент ми не маємо повністю самокерованих машин. 
Більшість цих проблем зараз {лягають=>лягає:::error_type=G/Number} на плечі розробників, проте ми не можемо точно сказати{=>,:::error_type=Punctuation} чи {їх=>їхні:::error_type=G/Other} рішення будуть справедливими, оскільки люди все ще лишаються людьми. Очевидно тільки одне, на даний момент спеціалісти {з=>зі:::error_type=Spelling} штучного інтелекту можуть вплинути на тільки на технологічну сферу, а також на етику та філософію. Всі описані вище проблеми потребують найшвидшого вирішення, інакше штучний інтелект може не виправдати сподівань людей і настане наступна технологічна зима.


