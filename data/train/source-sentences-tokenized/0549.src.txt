" Філософсько - етичні проблеми у сфері штучного інтелекту "
Питання життя та смерті - одвічне філософське питання , яке більшість людей вирішую по своєму .
А що , якщо я скажу що це питання потребує негайного і одноголосного рішення ?
За останні декілька років прогрес у сфері штучного інтелекту та машинного навчання зробив величезний крок вперед .
Більшість людей вже звикли до смартових речей у нашому житті — фітнес трекерів з вимірюванням пульсу , кардіограми та багатьох біометричних показників , смартфонів з розблокуванням сітківкою чи власним обличчям , для багатьох навіть роботи доставки Амазон на вулицях Силіконової долини звичайна річ .
Насправді процес сягає великих проривів в медицині .
Зараз системи розпізнавання зображень здатні визначити наявність певних видів раку чи наявність пневмонії з якістю кращою ніж кваліфіковані лікарі .
Більшість з нас не задумуються про етичні та філософські проблеми з якими стикаються спеціалісти з штучного інтелекту , які розробляють технологічні рішення .
У серпні цього року науковий співробітник з комп’ютерного зору та робототехніки університету Кембрідж Алекс Кендал опублікував роздум ” Let`s talk about Artificial intelligence ” , де він розглядав конкретні етичні проблеми спеціалістів з машинного навчання .
Три ключових проблеми , які він розглянув , це : довіра , чесність та справедливість .
Якщо чесність вимагає в основному легкого інтерпретування рішень , тобто вміння алгоритмом надати певні суттєві причини для деякого рішення , то з довірою та справедливістю все набагато складніше .
Справедливість .
Розглянемо доволі простий приклад .
Згідно з TrafficSTATS study by CMU for AAA ( 2007 ) чоловіки мають на 77 % вищій ризик померти в автомобільній аварії , ніж жінка .
Тобто , якщо подібні данні включають в модель яка визначає імовірність потрапити в аварію та вартість страхового полісу , вона матиме кращу точність , тобто буде справедливішою до реальної статистики .
Проте , згідно з рішенням Европейського суду справедливості у 2011 році , стать не може бути врахована у вартість страхового полісу як фактор ризику , бо вважається дескримінацією .
Іншими виявленими прикладами нечесних / необ’єктивних систем є : система розпізнавання голосу компанії Google систематично краще справляється з чоловічим голосом ніж з жіночим , система підрахунку ризику повторного правопорушення необ’єктивна щодо афро - американців .
Нажаль , системи штучного інтелекту мають властивість наслідувати упередженість від людей .
Вирішення цієї проблеми полягає в використанні більш якісних датасетів для кращого вивчення маленьких ( рідкісних ) класів , збільшенні якості методів збору та обробки даних , для зменшення упередженості даних .
Якщо зі справедливістю кроки доволі технічні , то з довірою все набагато складніше .
Критично важливим є те , що користувачі довіряють системам штучного інтелекту .
Якщо система не має належного рівня довіри , то ми навряд будемо її використовувати .
Для побудови безпечних систем машинного навчання , потрібно показати високу точність алгоритму , будувати алгоритми , які не тільки мають високу точність вирішення задачі , а й вміють визначати неточність своїх передбачень та розуміють , що є речі , які їм не відомі ( тобто , наприклад , використовувати Баєсівські моделі глибокого навчання ) .
Повертаючись до проблеми життя та смерті . у 2015 році видання MIT Technology review опублікувало на тему “ Why Self-Driving Cars Must Be Programmed to Kill ” . У тілі , якої представлено високо ймовірну ситуацію на дорозі , де хтось з великою ймовірністю загине лишається тільки обрати хто саме .
Саме тому , на даний момент ми не маємо повністю самокерованих машин .
Більшість цих проблем зараз лягають на плечі розробників , проте ми не можемо точно сказати чи їх рішення будуть справедливими , оскільки люди все ще лишаються людьми .
Очевидно тільки одне , на даний момент спеціалісти з штучного інтелекту можуть вплинути на тільки на технологічну сферу , а також на етику та філософію .
Всі описані вище проблеми потребують найшвидшого вирішення , інакше штучний інтелект може не виправдати сподівань людей і настане наступна технологічна зима .