"Філософсько-етичні проблеми у сфері штучного інтелекту"
Питання життя та смерті — одвічне філософське питання, яке більшість людей вирішує по-своєму.
А що коли я скажу, що це питання потребує негайного й одноголосного рішення?
За останні кілька років прогрес у сфері штучного інтелекту та машинного навчання зробив величезний крок уперед.
Більшість людей вже звикла до смартових речей у нашому житті — фітнес-трекерів з вимірюванням пульсу, кардіограми та багатьох біометричних показників, смартфонів з розблокуванням сітківкою чи власним обличчям, для багатьох навіть роботи доставки "Амазон" на вулицях Кремнієвої долини — звичайна річ.
Насправді процес сягає великих проривів у медицині.
Зараз системи розпізнавання зображень здатні визначити наявність певних видів раку чи наявність пневмонії краще за кваліфікованих лікарів.
Більшість із нас не задумується про етичні та філософські проблеми, з якими стикаються спеціалісти з штучного інтелекту, які розробляють технологічні рішення.
У серпні цього року науковий співробітник із комп’ютерного зору та робототехніки Кембриджського університету Алекс Кендал опублікував роздум ”Let`s talk about Artificial intelligence”, де  розглянув конкретні етичні проблеми спеціалістів із машинного навчання.
Три ключові проблеми, які він розглянув, це: довіра, чесність та справедливість.
Якщо чесність вимагає в основному легкого інтерпретування рішень, тобто вміння алгоритмом надати певні суттєві причини для деякого рішення, то з довірою та справедливістю все набагато складніше.
Справедливість.
Розглянемо доволі простий приклад.
Згідно з TrafficSTATS study by CMU for AAA (2007) чоловіки мають на 77% вищий ризик померти в автомобільній аварії, ніж жінка.
Тобто якщо подібні дані включають у модель, яка визначає ймовірність потрапити в аварію та вартість страхового полісу, вона матиме кращу точність, тобто буде справедливішою до реальної статистики.
Проте, згідно з рішенням Європейського суду справедливості у 2011 році, стать не може бути врахована у вартість страхового полісу як фактор ризику, бо вважатиметься дискримінацією.
Іншими виявленими прикладами нечесних / необ’єктивних систем є: система розпізнавання голосу компанії Google систематично краще справляється з чоловічим голосом, ніж із жіночим, система підрахунку ризику повторного правопорушення необ’єктивна щодо афроамериканців.
На жаль, системи штучного інтелекту мають властивість наслідувати упередженість від людей.
Вирішення цієї проблеми полягає в використанні якісніших датасетів для кращого вивчення маленьких (рідкісних) класів, збільшенні якості методів збору та обробки даних, для зменшення упередженості даних.
Якщо зі справедливістю кроки доволі технічні, то з довірою все набагато складніше.
Критично важливим є те, що користувачі довіряють системам штучного інтелекту.
Якщо система не має належного рівня довіри, то ми навряд її використовуватимемо.
Для побудови безпечних систем машинного навчання потрібно показати високу точність алгоритму, будувати алгоритми, які не тільки мають високу точність розв'язання завдання, а й уміють визначати неточність своїх передбачень і розуміють, що є речі, їм невідомі ( наприклад, використовувати Баєсівські моделі глибокого навчання).
Повертаючись до проблеми життя та смерті. У 2015 році видання "MIT Technology Review" опублікувало на тему “Why Self-Driving Cars Must Be Programmed to Kill”, у якій представлено високо ймовірну ситуацію на дорозі, де хтось з великою ймовірністю загине, лишається тільки обрати, хто саме.
Саме тому на цей момент ми не маємо повністю самокерованих машин.
Більшість цих проблем зараз лягає на плечі розробників, проте ми не можемо точно сказати, чи їхні рішення будуть справедливими, оскільки люди все ще лишаються людьми.
Очевидно тільки одне, на цей момент спеціалісти з штучного інтелекту можуть вплинути на тільки на технологічну сферу, а також на етику та філософію.
Всі описані вище проблеми потребують найшвидшого вирішення, інакше штучний інтелект може не виправдати сподівань людей і настане наступна технологічна зима.